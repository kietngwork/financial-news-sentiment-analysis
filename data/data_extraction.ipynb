{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    # Ensure 'Date' column is in datetime format\n",
    "    chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n",
    "    \n",
    "    # Filter data for the year 2018\n",
    "    chunk_2018 = chunk[chunk['Date'].dt.year == 2018]\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    chunk_2018 = chunk_2018.dropna(subset=['Date', 'Article_title', 'Stock_symbol'])\n",
    "    \n",
    "    # Retain only the specified columns\n",
    "    chunk_2018 = chunk_2018[['Date', 'Article_title', 'Stock_symbol']]\n",
    "    \n",
    "    print(f\"Processed chunk with {len(chunk)} rows, filtered to {len(chunk_2018)} rows\")\n",
    "    \n",
    "    return chunk_2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_large_csv(file_path, chunksize=100000):\n",
    "    # Specify the data types for each column\n",
    "    dtype = {\n",
    "        'Date': 'str',  # Read as string and convert to datetime later\n",
    "        'Article_title': 'str',\n",
    "        'Stock_symbol': 'str'\n",
    "    }\n",
    "    \n",
    "    chunk_iter = pd.read_csv(file_path, chunksize=chunksize, dtype=dtype)\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunk_iter:\n",
    "        filtered_chunk = process_chunk(chunk)\n",
    "        if not filtered_chunk.empty:\n",
    "            filtered_chunks.append(filtered_chunk)\n",
    "    \n",
    "    # Concatenate all filtered chunks into a single DataFrame\n",
    "    if filtered_chunks:\n",
    "        filtered_df = pd.concat(filtered_chunks, ignore_index=True)\n",
    "    else:\n",
    "        filtered_df = pd.DataFrame(columns=['Date', 'Article_title', 'Stock_symbol'])\n",
    "    \n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk with 100000 rows, filtered to 6179 rows\n",
      "Processed chunk with 100000 rows, filtered to 7421 rows\n",
      "Processed chunk with 100000 rows, filtered to 7715 rows\n",
      "Processed chunk with 100000 rows, filtered to 5414 rows\n",
      "Processed chunk with 100000 rows, filtered to 6889 rows\n",
      "Processed chunk with 100000 rows, filtered to 6810 rows\n",
      "Processed chunk with 100000 rows, filtered to 8471 rows\n",
      "Processed chunk with 100000 rows, filtered to 5594 rows\n",
      "Processed chunk with 100000 rows, filtered to 7376 rows\n",
      "Processed chunk with 100000 rows, filtered to 5422 rows\n",
      "Processed chunk with 100000 rows, filtered to 5983 rows\n",
      "Processed chunk with 100000 rows, filtered to 6160 rows\n",
      "Processed chunk with 100000 rows, filtered to 5079 rows\n",
      "Processed chunk with 100000 rows, filtered to 7222 rows\n",
      "Processed chunk with 100000 rows, filtered to 8442 rows\n",
      "Processed chunk with 100000 rows, filtered to 8720 rows\n",
      "Processed chunk with 100000 rows, filtered to 4563 rows\n",
      "Processed chunk with 100000 rows, filtered to 7225 rows\n",
      "Processed chunk with 100000 rows, filtered to 7573 rows\n",
      "Processed chunk with 100000 rows, filtered to 9198 rows\n",
      "Processed chunk with 100000 rows, filtered to 10772 rows\n",
      "Processed chunk with 100000 rows, filtered to 5546 rows\n",
      "Processed chunk with 100000 rows, filtered to 7918 rows\n",
      "Processed chunk with 100000 rows, filtered to 6725 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/qbwgkp_97lj_qtmnz0f8hj100000gn/T/ipykernel_27814/918978720.py:11: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in chunk_iter:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk with 100000 rows, filtered to 7510 rows\n",
      "Processed chunk with 100000 rows, filtered to 10833 rows\n",
      "Processed chunk with 100000 rows, filtered to 10300 rows\n",
      "Processed chunk with 100000 rows, filtered to 10807 rows\n",
      "Processed chunk with 100000 rows, filtered to 9428 rows\n",
      "Processed chunk with 100000 rows, filtered to 10756 rows\n",
      "Processed chunk with 100000 rows, filtered to 9614 rows\n",
      "Processed chunk with 100000 rows, filtered to 11114 rows\n",
      "Processed chunk with 100000 rows, filtered to 10063 rows\n",
      "Processed chunk with 100000 rows, filtered to 10827 rows\n",
      "Processed chunk with 100000 rows, filtered to 11011 rows\n",
      "Processed chunk with 100000 rows, filtered to 9669 rows\n",
      "Processed chunk with 100000 rows, filtered to 9745 rows\n",
      "Processed chunk with 100000 rows, filtered to 10580 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/qbwgkp_97lj_qtmnz0f8hj100000gn/T/ipykernel_27814/918978720.py:11: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in chunk_iter:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk with 100000 rows, filtered to 11364 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/qbwgkp_97lj_qtmnz0f8hj100000gn/T/ipykernel_27814/918978720.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in chunk_iter:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk with 100000 rows, filtered to 0 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/qbwgkp_97lj_qtmnz0f8hj100000gn/T/ipykernel_27814/918978720.py:11: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in chunk_iter:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk with 100000 rows, filtered to 17450 rows\n",
      "Processed chunk with 100000 rows, filtered to 17658 rows\n",
      "Processed chunk with 100000 rows, filtered to 17036 rows\n",
      "Processed chunk with 100000 rows, filtered to 17015 rows\n",
      "Processed chunk with 100000 rows, filtered to 19161 rows\n",
      "Processed chunk with 100000 rows, filtered to 21999 rows\n",
      "Processed chunk with 100000 rows, filtered to 18621 rows\n",
      "Processed chunk with 100000 rows, filtered to 17801 rows\n",
      "Processed chunk with 100000 rows, filtered to 18050 rows\n",
      "Processed chunk with 100000 rows, filtered to 18208 rows\n",
      "Processed chunk with 100000 rows, filtered to 18417 rows\n",
      "Processed chunk with 100000 rows, filtered to 17384 rows\n",
      "Processed chunk with 100000 rows, filtered to 18444 rows\n",
      "Processed chunk with 100000 rows, filtered to 16460 rows\n",
      "Processed chunk with 100000 rows, filtered to 16873 rows\n",
      "Processed chunk with 100000 rows, filtered to 15967 rows\n",
      "Processed chunk with 100000 rows, filtered to 18890 rows\n",
      "Processed chunk with 100000 rows, filtered to 18231 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/qbwgkp_97lj_qtmnz0f8hj100000gn/T/ipykernel_27814/918978720.py:11: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in chunk_iter:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk with 100000 rows, filtered to 8936 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 100000 rows, filtered to 0 rows\n",
      "Processed chunk with 49299 rows, filtered to 0 rows\n",
      "Final dataset has 654639 rows\n",
      "                       Date                                Article_title  \\\n",
      "0 2018-11-26 00:00:00+00:00     Asian ADRs Move Higher in Monday Trading   \n",
      "1 2018-11-14 00:00:00+00:00  Asian ADRs Move Higher in Wednesday Trading   \n",
      "2 2018-11-09 00:00:00+00:00      Asian ADRs Move Lower in Friday Trading   \n",
      "3 2018-10-26 00:00:00+00:00      Asian ADRs Move Lower in Friday Trading   \n",
      "4 2018-10-25 00:00:00+00:00   Asian ADRs Move Higher in Thursday Trading   \n",
      "\n",
      "  Stock_symbol  \n",
      "0         AACG  \n",
      "1         AACG  \n",
      "2         AACG  \n",
      "3         AACG  \n",
      "4         AACG  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/Users/ngtnkiet/Downloads/hello_ds/nasdaq_exteral_data.csv\"  # Update this path to your CSV file\n",
    "    df = load_large_csv(file_path)\n",
    "    print(f\"Final dataset has {len(df)} rows\")\n",
    "    print(df.head())  # Print the first few rows of the final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 654639 entries, 0 to 654638\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count   Dtype              \n",
      "---  ------         --------------   -----              \n",
      " 0   Date           654639 non-null  datetime64[ns, UTC]\n",
      " 1   Article_title  654639 non-null  object             \n",
      " 2   Stock_symbol   654639 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), object(2)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "article_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "stock_symbol",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ec199345-9a16-4b3c-b0aa-6544aee6831c",
       "rows": [
        [
         "0",
         "2018-11-26 00:00:00+00:00",
         "Asian ADRs Move Higher in Monday Trading",
         "AACG"
        ],
        [
         "1",
         "2018-11-14 00:00:00+00:00",
         "Asian ADRs Move Higher in Wednesday Trading",
         "AACG"
        ],
        [
         "2",
         "2018-11-09 00:00:00+00:00",
         "Asian ADRs Move Lower in Friday Trading",
         "AACG"
        ],
        [
         "3",
         "2018-10-26 00:00:00+00:00",
         "Asian ADRs Move Lower in Friday Trading",
         "AACG"
        ],
        [
         "4",
         "2018-10-25 00:00:00+00:00",
         "Asian ADRs Move Higher in Thursday Trading",
         "AACG"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article_title</th>\n",
       "      <th>stock_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-26 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Higher in Monday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-14 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Higher in Wednesday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-09 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Lower in Friday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-26 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Lower in Friday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-25 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Higher in Thursday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date                                article_title  \\\n",
       "0 2018-11-26 00:00:00+00:00     Asian ADRs Move Higher in Monday Trading   \n",
       "1 2018-11-14 00:00:00+00:00  Asian ADRs Move Higher in Wednesday Trading   \n",
       "2 2018-11-09 00:00:00+00:00      Asian ADRs Move Lower in Friday Trading   \n",
       "3 2018-10-26 00:00:00+00:00      Asian ADRs Move Lower in Friday Trading   \n",
       "4 2018-10-25 00:00:00+00:00   Asian ADRs Move Higher in Thursday Trading   \n",
       "\n",
       "  stock_symbol  \n",
       "0         AACG  \n",
       "1         AACG  \n",
       "2         AACG  \n",
       "3         AACG  \n",
       "4         AACG  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert column names to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 654639 entries, 0 to 654638\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count   Dtype              \n",
      "---  ------         --------------   -----              \n",
      " 0   date           654639 non-null  datetime64[ns, UTC]\n",
      " 1   article_title  654639 non-null  object             \n",
      " 2   stock_symbol   654639 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), object(2)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Convert 'DateColumn' to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to CSV file\n",
    "df.to_csv('cleaned_2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "article_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "stock_symbol",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "53e58527-b6c4-4254-adeb-566d8e07fda5",
       "rows": [
        [
         "0",
         "2018-11-26 00:00:00+00:00",
         "Asian ADRs Move Higher in Monday Trading",
         "AACG"
        ],
        [
         "1",
         "2018-11-14 00:00:00+00:00",
         "Asian ADRs Move Higher in Wednesday Trading",
         "AACG"
        ],
        [
         "2",
         "2018-11-09 00:00:00+00:00",
         "Asian ADRs Move Lower in Friday Trading",
         "AACG"
        ],
        [
         "3",
         "2018-10-26 00:00:00+00:00",
         "Asian ADRs Move Lower in Friday Trading",
         "AACG"
        ],
        [
         "4",
         "2018-10-25 00:00:00+00:00",
         "Asian ADRs Move Higher in Thursday Trading",
         "AACG"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article_title</th>\n",
       "      <th>stock_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-26 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Higher in Monday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-14 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Higher in Wednesday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-09 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Lower in Friday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-26 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Lower in Friday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-25 00:00:00+00:00</td>\n",
       "      <td>Asian ADRs Move Higher in Thursday Trading</td>\n",
       "      <td>AACG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date                                article_title  \\\n",
       "0 2018-11-26 00:00:00+00:00     Asian ADRs Move Higher in Monday Trading   \n",
       "1 2018-11-14 00:00:00+00:00  Asian ADRs Move Higher in Wednesday Trading   \n",
       "2 2018-11-09 00:00:00+00:00      Asian ADRs Move Lower in Friday Trading   \n",
       "3 2018-10-26 00:00:00+00:00      Asian ADRs Move Lower in Friday Trading   \n",
       "4 2018-10-25 00:00:00+00:00   Asian ADRs Move Higher in Thursday Trading   \n",
       "\n",
       "  stock_symbol  \n",
       "0         AACG  \n",
       "1         AACG  \n",
       "2         AACG  \n",
       "3         AACG  \n",
       "4         AACG  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
